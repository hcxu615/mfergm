{
    "contents" : "#########################################################\n#\n# program: mc_bias.R\n# author: Angelo Mele\n# date: 02/01/2014\n# last modified: 02/07/2014\n# description: Monte Carlo for testing the bias of\n#              MF-MLE\n#\n#########################################################\n\n\n# cancel all variables from the memory\nrm(list=ls())\n\n# libraries\nlibrary(rootSolve)\n\n\n# # set working directory (change this)\n#  setwd(\"C:/Users/amele1/Dropbox/peersmoking/programs/datagen/datagen/Debug\")\n# # \n# # run fortran code to generate datasets\n#  system(\"datagen.exe\")\n\n# data url: where the artificial data are located\ndataurl <- \"C:/Users/amele1/Dropbox/peersmoking/results/\"\n\n#############################################################\n#####   call functions for the approximations\n############################################################\n\n# variational log likelihood function    \nvarloglik <- function(theta) {\n  #initialize mu\n  #mu<-runif(n,0,1)\n  #mu<-rep(1/(1+exp(-theta[1])),n)\n  set.seed(0)\n  mu<-runif(n)\n  xi<-log(mu/(1-mu))\n  kmu<-c()\n  kmu[1]<-theta[1]*sum(1/(1+exp(-xi))) +\n    .5*theta[2]*t(1/(1+exp(-xi))) %*% g %*% (1/(1+exp(-xi)))/(n-1) -\n    sum( log(1+exp(xi)) - xi/(1+exp(-xi)) )\n  # find approximation\n  eps <- 10\n  tol<-0.00001\n  iter<-1\n  i<-1\n  while (eps >= tol) {\n    #if(eps<0) break\n    # xi update\n    xi.new <- theta[1] + theta[2]* g[i,] %*% (1/(1+exp(-xi)))/(n-1) \n    xi.new <- as.numeric(xi.new)\n    # compute bound \n    eps <- xi.new  *(1/(1+exp(-xi.new)) - 1/(1+exp(-xi[i])) )\n    -( log(1+exp(xi.new)) - log(1+exp(xi[i]))\n       + (xi.new/(1+exp(-xi.new)) - xi[i]/(1+exp(-xi[i])) )  )\n    eps <- as.numeric(eps)\n    kmu[iter+1] <- kmu[iter] + eps\n#     kmu[iter+1]<-kmu[iter] +\n#       xi.new  *(1/(1+exp(-xi.new)) - 1/(1+exp(-xi[i])) )\n#     -( log(1+exp(xi.new)) - log(1+exp(xi[i]))\n#       + (xi.new/(1+exp(-xi.new)) - xi[i]/(1+exp(-xi[i])) )  )\n#    eps<- kmu[iter+1]-kmu[iter]  # compute change in bound\n    xi[i] <- xi.new   # update xi[i]\n    #print(paste(\"iter= \", iter))\n    #print(eps)\n    #print(kmu[iter])\n    #print(kmu[iter+1])\n    iter<-iter+1\n    i<-i+1\n    #print(i)\n    if (i==n+1) i<-1\n    \n  }\n  \n  \n  #if (kmu[iter]==\"NaN\") {kmu[iter]<- 100000000000}\n  varloglik<- sumy*theta[1] + .5*theta[2]*ygy/(n-1) - kmu[iter]\n  #print(iter)\n  #print(kmu[iter])\n  #print(varloglik)\n  return<- varloglik \n}\n\n\n\nvarloglik <- function(theta) {\n  mu <- rep(1/(1+exp(-theta[1])), n)\n  kmu<-c()\n  kmu[1]<-theta[1]*sum(mu) +\n    .5*theta[2]*t(mu) %*% g %*% mu /(n-1) -\n    sum( mu*log(mu) +  (1-mu)*log(1-mu) )\n  # find approximation\n  eps <- 10\n  tol<-0.00001\n  iter<-1\n  i<-1\n  while (eps >= tol) {\n    if (i == n+1) i<-1\n    # update mu_i\n    mui <- as.numeric( 1/ (1+ exp(-theta[1] - theta[2]*g[i,] %*% mu /(n-1) )) )\n    eps <- as.numeric( theta[1]*(mui-mu[i]) +\n            theta[2]*g[i,] %*% mu *(mui - mu[i]) /(n-1) -\n            ( mui*log(mui) + (1-mui)*log(1-mui) -\n              mu[i]*log(mu[i]) - (1-mu[i])*log(1-mu[i]) )  )\n    print(eps)\n    kmu[iter+1] <- kmu[iter] + eps\n    mu[i] <- mui\n    iter <- iter+1\n    i <- i + 1\n  }\n  varloglik <- theta[1]*sumy + .5* theta[2] * ygy/(n-1) - kmu[iter]\n  return(varloglik)\n}\n\n\n# global optimization for kmu\ngkmu <- function(xi) {\n  #xi<-log(mu/(1-mu))\n  gkmu<-theta[1]*sum(1/(1+exp(-xi))) +\n    .5*theta[2]*t(1/(1+exp(-xi))) %*% g %*% (1/(1+exp(-xi)))/(n-1) -\n    sum( log(1+exp(xi)) - xi/(1+exp(-xi)) )\n  return(gkmu)\n}\n\ngvarloglik <- function(theta) {\n  mu <- rep(1/(1+exp(-theta[1])), n)\n  xi<-log(mu/(1-mu))\n  res <- optim(xi, gkmu, method=\"Nelder-Mead\", control=list(fnscale=-1, maxit=2000))\n  kmu <- res$value\n  gvarloglik <- theta[1]*sumy + .5* theta[2] * ygy/(n-1) - kmu\n  return(gvarloglik)\n}\n\n\n# diaconis approximation\n# TBA\n\n# initialize vectors\nest.pars <- matrix(NA, nrow=100, ncol=2)\nloglik <- array(NA,100)\n\n\n\n\nn <- 1000\np <- 10\nfor (ss in 1:5) {\n  s <- ss\n  print(ss)\n  \n  # read files with data\n  filename<-paste(dataurl,\"y_n\",n,\"_p\",p,\"_s\",s,\".txt\", sep=\"\")\n  filename\n  y<-scan(filename)\n  filename<-paste(dataurl,\"g_n\",n,\"_p\",p,\".txt\", sep=\"\")\n  filename\n  gg<-scan(filename)\n  g<-matrix(gg,n,n)\n  g<-t(g)\n  \n  # data summaries\n  sumy<-sum(y)\n  ygy <- as.numeric( t(y) %*% g %*% y )\n  e<-sum(g)\n  ebar<-e/(n-1)\n  \n  # find max likelihood estimate\n  theta<-c(-1,5)\n  #res <- optim(theta, varloglik, method=\"Nelder-Mead\", control=list(fnscale=-1))\n  #res <- optim(theta, varloglik, method=\"SANN\", control=list(fnscale=-1))\n  res <- optim(theta, varloglik, method=\"L-BFGS-B\", control=list(fnscale=-1))\n  \n  # store results in vectors\n  est.pars[ss,] <- res$par\n  loglik[ss] <- res$value\n}\n\nmse1<- mean((est.pars[,1] - (-1) )^2)\nmse2<- mean((est.pars[,2] - (5) )^2)\nmse1\nmse2\n\nbias1 <- mean(est.pars[,1]) - (-1)\nbias2 <- mean(est.pars[,2]) - 5\nbias1\nbias2\n\nbias1pct <- (mean(est.pars[,1]) - (-1))/(-1)\nbias2pct <- (mean(est.pars[,2]) - 5)/5\nbias1pct\nbias2pct\n\ncbind(est.pars[1:5,], loglik[1:5])\ncbind(est.pars, loglik)\n\nloglik[1:5,]\n\n\n\n\n\n\n\n\n\n\n\n### trials with global optim\n# initialize vectors\nest.pars <- matrix(NA, nrow=100, ncol=2)\nloglik <- array(NA,100)\n\nn <- 100\np <- 10\nfor (ss in 1:5) {\n  s <- ss\n  print(ss)\n  \n  # read files with data\n  filename<-paste(dataurl,\"y_n\",n,\"_p\",p,\"_s\",s,\".txt\", sep=\"\")\n  filename\n  y<-scan(filename)\n  filename<-paste(dataurl,\"g_n\",n,\"_p\",p,\".txt\", sep=\"\")\n  filename\n  gg<-scan(filename)\n  g<-matrix(gg,n,n)\n  g<-t(g)\n  \n  # data summaries\n  sumy<-sum(y)\n  ygy <- as.numeric( t(y) %*% g %*% y )\n  e<-sum(g)\n  ebar<-e/(n-1)\n  \n  # find max likelihood estimate\n  theta<-c(-1,5)\n  res <- optim(theta, gvarloglik, method=\"Nelder-Mead\", control=list(fnscale=-1))\n  #res <- optim(theta, varloglik, method=\"SANN\", control=list(fnscale=-1))\n  \n  # store results in vectors\n  est.pars[ss,] <- res$par\n  loglik[ss] <- res$value\n}\n\nmse1<- mean((est.pars[,1] - (-1) )^2)\nmse2<- mean((est.pars[,2] - (5) )^2)\nmse1\nmse2\n\ncbind(est.pars[1:5,], loglik[1:5])\ncbind(est.pars, loglik)\n\nloglik[1:5,]\n\n\n\n\n\n\n\n\n\n#### parallelizing the var log lik function\n\n#########################################################\n#\n# program: mc_bias.R\n# author: Angelo Mele\n# date: 02/01/2014\n# last modified: 02/07/2014\n# description: Monte Carlo for testing the bias of\n#              MF-MLE\n#\n#########################################################\n\n\n# cancel all variables from the memory\nrm(list=ls())\n\n# libraries\nlibrary(rootSolve)\n\n\n# # set working directory (change this)\n#  setwd(\"C:/Users/amele1/Dropbox/peersmoking/programs/datagen/datagen/Debug\")\n# # \n# # run fortran code to generate datasets\n#  system(\"datagen.exe\")\n\n# data url: where the artificial data are located\ndataurl <- \"C:/Users/amele1/Dropbox/peersmoking/results/\"\n\n#############################################################\n#####   call functions for the approximations\n############################################################\n\n# function that computes bound for log partition\n#kmufcn <- function(k, theta, n, g) {\nkmufcn <- function(k) {\n    set.seed(k*10)\n  print(\"point 1\")\n  mu<-runif(n)\n  xi<-log(mu/(1-mu))\n  kmu<-c()\n  print(\"point 2\")\n  kmu[1]<-theta[1]*sum(1/(1+exp(-xi))) +\n    .5*theta[2]*t(1/(1+exp(-xi))) %*% g %*% (1/(1+exp(-xi)))/(n-1) -\n    sum( log(1+exp(xi)) - xi/(1+exp(-xi)) )\n  # find approximation\n  eps <- 10\n  tol<-0.00001\n  iter<-1\n  i<-1\n  while (eps >= tol) {\n    #if(eps<0) break\n    # xi update\n    xi.new <- theta[1] + theta[2]* g[i,] %*% (1/(1+exp(-xi)))/(n-1) \n    xi.new <- as.numeric(xi.new)\n    # compute bound \n    eps <- xi.new  *(1/(1+exp(-xi.new)) - 1/(1+exp(-xi[i])) )\n    -( log(1+exp(xi.new)) - log(1+exp(xi[i]))\n       + (xi.new/(1+exp(-xi.new)) - xi[i]/(1+exp(-xi[i])) )  )\n    eps <- as.numeric(eps)\n    kmu[iter+1] <- kmu[iter] + eps\n    #     kmu[iter+1]<-kmu[iter] +\n    #       xi.new  *(1/(1+exp(-xi.new)) - 1/(1+exp(-xi[i])) )\n    #     -( log(1+exp(xi.new)) - log(1+exp(xi[i]))\n    #       + (xi.new/(1+exp(-xi.new)) - xi[i]/(1+exp(-xi[i])) )  )\n    #    eps<- kmu[iter+1]-kmu[iter]  # compute change in bound\n    xi[i] <- xi.new   # update xi[i]\n    #print(paste(\"iter= \", iter))\n    #print(eps)\n    #print(kmu[iter])\n    #print(kmu[iter+1])\n    iter<-iter+1\n    i<-i+1\n    #print(i)\n    if (i==n+1) i<-1\n    \n  }\n  return(kmu[iter])\n  \n}\n\nlibrary(parallel)\npvarloglik <- function(theta, nstart, nproc, n, g, sumy, ygy) {\n  require(parallel)\n  start<- 1:nstart\n  #cl<-makeCluster(nproc, type=\"PSOCK\")\n  cl<- makeCluster(mc <- getOption(\"cl.cores\", 2))\n  #kmf<-rep(NA,nstart)\n  #kmf<-parLapply(cl, start , kmufcn, theta=theta, n=n,  g=g)\n  kmf<-parLapply(cl, start , kmufcn)\n  stopCluster(cl)\n  km<-max(kmf)\n  pvarloglik<- sumy*theta[1] + .5*theta[2]*ygy/(n-1) - km\n  return(pvarloglik)\n}\n\n\n\n\n#### USING FOREACH\nlibrary(foreach)\n# function that computes bound for log partition\n#kmufcn <- function(k, theta, n, g) {\nkmufcn <- function(k) {\n  set.seed(k*10)\n  #print(\"point 1\")\n  mu<-runif(n)\n  xi<-log(mu/(1-mu))\n  kmu<-c()\n  #print(\"point 2\")\n  kmu[1]<-theta[1]*sum(1/(1+exp(-xi))) +\n    .5*theta[2]*t(1/(1+exp(-xi))) %*% g %*% (1/(1+exp(-xi)))/(n-1) -\n    sum( log(1+exp(xi)) - xi/(1+exp(-xi)) )\n  # find approximation\n  eps <- 10\n  tol<-0.00001\n  iter<-1\n  i<-1\n  while (eps >= tol) {\n    #if(eps<0) break\n    # xi update\n    xi.new <- theta[1] + theta[2]* g[i,] %*% (1/(1+exp(-xi)))/(n-1) \n    xi.new <- as.numeric(xi.new)\n    # compute bound \n    eps <- xi.new  *(1/(1+exp(-xi.new)) - 1/(1+exp(-xi[i])) )\n    -( log(1+exp(xi.new)) - log(1+exp(xi[i]))\n       + (xi.new/(1+exp(-xi.new)) - xi[i]/(1+exp(-xi[i])) )  )\n    eps <- as.numeric(eps)\n    kmu[iter+1] <- kmu[iter] + eps\n    #     kmu[iter+1]<-kmu[iter] +\n    #       xi.new  *(1/(1+exp(-xi.new)) - 1/(1+exp(-xi[i])) )\n    #     -( log(1+exp(xi.new)) - log(1+exp(xi[i]))\n    #       + (xi.new/(1+exp(-xi.new)) - xi[i]/(1+exp(-xi[i])) )  )\n    #    eps<- kmu[iter+1]-kmu[iter]  # compute change in bound\n    xi[i] <- xi.new   # update xi[i]\n    #print(paste(\"iter= \", iter))\n    #print(eps)\n    #print(kmu[iter])\n    #print(kmu[iter+1])\n    iter<-iter+1\n    i<-i+1\n    #print(i)\n    if (i==n+1) i<-1\n    \n  }\n  return(kmu[iter])\n  \n}\n\npvarloglik <- function(theta, nstart) {\n  \n  \n  \n  #kmf<-rep(NA,nstart)\n  #kmf<-parLapply(cl, start , kmufcn, theta=theta, n=n,  g=g)\n  kmf<- foreach(k=1:nstart, .combine='c') %do% kmufcn(k)\n  km<-max(kmf)\n  pvarloglik<- sumy*theta[1] + .5*theta[2]*ygy/(n-1) - km\n  return(pvarloglik)\n}\n\n\n\n# variational log likelihood function    \nvarloglik <- function(theta) {\n  #initialize mu\n  #mu<-runif(n,0,1)\n  #mu<-rep(1/(1+exp(-theta[1])),n)\n  set.seed(0)\n  mu<-runif(n)\n  xi<-log(mu/(1-mu))\n  kmu<-c()\n  kmu[1]<-theta[1]*sum(1/(1+exp(-xi))) +\n    .5*theta[2]*t(1/(1+exp(-xi))) %*% g %*% (1/(1+exp(-xi)))/(n-1) -\n    sum( log(1+exp(xi)) - xi/(1+exp(-xi)) )\n  # find approximation\n  eps <- 10\n  tol<-0.00001\n  iter<-1\n  i<-1\n  while (eps >= tol) {\n    #if(eps<0) break\n    # xi update\n    xi.new <- theta[1] + theta[2]* g[i,] %*% (1/(1+exp(-xi)))/(n-1) \n    xi.new <- as.numeric(xi.new)\n    # compute bound \n    eps <- xi.new  *(1/(1+exp(-xi.new)) - 1/(1+exp(-xi[i])) )\n    -( log(1+exp(xi.new)) - log(1+exp(xi[i]))\n       + (xi.new/(1+exp(-xi.new)) - xi[i]/(1+exp(-xi[i])) )  )\n    eps <- as.numeric(eps)\n    kmu[iter+1] <- kmu[iter] + eps\n    #     kmu[iter+1]<-kmu[iter] +\n    #       xi.new  *(1/(1+exp(-xi.new)) - 1/(1+exp(-xi[i])) )\n    #     -( log(1+exp(xi.new)) - log(1+exp(xi[i]))\n    #       + (xi.new/(1+exp(-xi.new)) - xi[i]/(1+exp(-xi[i])) )  )\n    #    eps<- kmu[iter+1]-kmu[iter]  # compute change in bound\n    xi[i] <- xi.new   # update xi[i]\n    #print(paste(\"iter= \", iter))\n    #print(eps)\n    #print(kmu[iter])\n    #print(kmu[iter+1])\n    iter<-iter+1\n    i<-i+1\n    #print(i)\n    if (i==n+1) i<-1\n    \n  }\n  \n  \n  #if (kmu[iter]==\"NaN\") {kmu[iter]<- 100000000000}\n  varloglik<- sumy*theta[1] + .5*theta[2]*ygy/(n-1) - kmu[iter]\n  #print(iter)\n  #print(kmu[iter])\n  #print(varloglik)\n  return<- varloglik \n}\n\n\n\n# initialize vectors\nest.pars <- matrix(NA, nrow=100, ncol=2)\nloglik <- array(NA,100)\n\n\n\n\nn <- 100\np <- 10\nfor (ss in 1:5) {\n  s <- ss\n  print(ss)\n  \n  # read files with data\n  filename<-paste(dataurl,\"y_n\",n,\"_p\",p,\"_s\",s,\".txt\", sep=\"\")\n  filename\n  y<-scan(filename)\n  filename<-paste(dataurl,\"g_n\",n,\"_p\",p,\".txt\", sep=\"\")\n  filename\n  gg<-scan(filename)\n  g<-matrix(gg,n,n)\n  g<-t(g)\n  \n  # data summaries\n  sumy<-sum(y)\n  ygy <- as.numeric( t(y) %*% g %*% y )\n  e<-sum(g)\n  ebar<-e/(n-1)\n  \n  # find max likelihood estimate\n  theta<-c(-1,5)\n  #res <- optim(theta, varloglik, method=\"Nelder-Mead\", control=list(fnscale=-1))\n  #res <- optim(theta, varloglik, method=\"SANN\", control=list(fnscale=-1))\n  nproc<-2\n  nstart<-5\n  #res <- optim(theta, pvarloglik, method=\"L-BFGS-B\", control=list(fnscale=-1)\n  #             , nproc=nproc, nstart=nstart, n=n, g=g, sumy-sumy, ygy=ygy)\n  res <- optim(theta, pvarloglik, method=\"L-BFGS-B\", control=list(fnscale=-1), nstart=nstart)\n  \n  # store results in vectors\n  est.pars[ss,] <- res$par\n  loglik[ss] <- res$value\n}\n\nmse1<- mean((est.pars[,1] - (-1) )^2)\nmse2<- mean((est.pars[,2] - (5) )^2)\nmse1\nmse2\n\nbias1 <- mean(est.pars[,1]) - (-1)\nbias2 <- mean(est.pars[,2]) - 5\nbias1\nbias2\n\nbias1pct <- (mean(est.pars[,1]) - (-1))/(-1)\nbias2pct <- (mean(est.pars[,2]) - 5)/5\nbias1pct\nbias2pct\n\ncbind(est.pars[1:5,], loglik[1:5])\ncbind(est.pars, loglik)\n\nloglik[1:5,]\n\n\n\n\n\n\n\n#########################################################\n#\n# program: mc_bias.R\n# author: Angelo Mele\n# date: 02/01/2014\n# last modified: 02/07/2014\n# description: Monte Carlo for testing the bias of\n#              MF-MLE\n#\n#########################################################\n\n\n# cancel all variables from the memory\nrm(list=ls())\n\n# libraries\nlibrary(rootSolve)\n\n\n# # set working directory (change this)\n#  setwd(\"C:/Users/amele1/Dropbox/peersmoking/programs/datagen/datagen/Debug\")\n# # \n# # run fortran code to generate datasets\n#  system(\"datagen.exe\")\n\n# data url: where the artificial data are located\ndataurl <- \"C:/Users/amele1/Dropbox/peersmoking/results/\"\n\n#############################################################\n#####   call functions for the approximations\n############################################################\n\n\n\n\n# variational log likelihood function    \nvarloglik <- function(theta, nstart) {\n  #initialize mu\n  #mu<-runif(n,0,1)\n  #mu<-rep(1/(1+exp(-theta[1])),n)\n  kmf <- rep(NA, nstart)\n  for (k in 1:nstart) {\n    set.seed(k*1000)\n    mu<-runif(n)\n    xi<-log(mu/(1-mu))\n    kmu<-c()\n    kmu[1]<-theta[1]*sum(1/(1+exp(-xi))) +\n      .5*theta[2]*t(1/(1+exp(-xi))) %*% g %*% (1/(1+exp(-xi)))/(n-1) -\n      sum( log(1+exp(xi)) - xi/(1+exp(-xi)) )\n    # find approximation\n    eps <- 10\n    tol<-0.00001\n    iter<-1\n    i<-1\n    while (eps >= tol) {\n      #if(eps<0) break\n      # xi update\n      xi.new <- theta[1] + theta[2]* g[i,] %*% (1/(1+exp(-xi)))/(n-1) \n      xi.new <- as.numeric(xi.new)\n      # compute bound \n      eps <- xi.new  *(1/(1+exp(-xi.new)) - 1/(1+exp(-xi[i])) )\n      -( log(1+exp(xi.new)) - log(1+exp(xi[i]))\n         + (xi.new/(1+exp(-xi.new)) - xi[i]/(1+exp(-xi[i])) )  )\n      eps <- as.numeric(eps)\n      kmu[iter+1] <- kmu[iter] + eps\n      #     kmu[iter+1]<-kmu[iter] +\n      #       xi.new  *(1/(1+exp(-xi.new)) - 1/(1+exp(-xi[i])) )\n      #     -( log(1+exp(xi.new)) - log(1+exp(xi[i]))\n      #       + (xi.new/(1+exp(-xi.new)) - xi[i]/(1+exp(-xi[i])) )  )\n      #    eps<- kmu[iter+1]-kmu[iter]  # compute change in bound\n      xi[i] <- xi.new   # update xi[i]\n      #print(paste(\"iter= \", iter))\n      #print(eps)\n      #print(kmu[iter])\n      #print(kmu[iter+1])\n      iter<-iter+1\n      i<-i+1\n      #print(i)\n      if (i==n+1) i<-1\n      \n    }\n  kmf[k]<-kmu[iter]\n  }\n  #print(kmf)\n  km <- max(kmf)\n  #if (kmu[iter]==\"NaN\") {kmu[iter]<- 100000000000}\n  varloglik<- sumy*theta[1] + .5*theta[2]*ygy/(n-1) - km\n  #print(iter)\n  #print(kmu[iter])\n  #print(varloglik)\n  return<- varloglik \n}\n\n# diaconis approximation\n# TBA\n\n# initialize vectors\nest.pars <- matrix(NA, nrow=100, ncol=2)\nloglik <- array(NA,100)\n\n\n\n\nn <- 1000\np <- 10\nfor (ss in 1:5) {\n  s <- ss\n  print(ss)\n  \n  # read files with data\n  filename<-paste(dataurl,\"y_n\",n,\"_p\",p,\"_s\",s,\".txt\", sep=\"\")\n  filename\n  y<-scan(filename)\n  filename<-paste(dataurl,\"g_n\",n,\"_p\",p,\".txt\", sep=\"\")\n  filename\n  gg<-scan(filename)\n  g<-matrix(gg,n,n)\n  g<-t(g)\n  \n  # data summaries\n  sumy<-sum(y)\n  ygy <- as.numeric( t(y) %*% g %*% y )\n  e<-sum(g)\n  ebar<-e/(n-1)\n  \n  # find max likelihood estimate\n  theta<-c(-1,5)\n  nstart<-5\n  #res <- optim(theta, varloglik, method=\"Nelder-Mead\", control=list(fnscale=-1), nstart=nstart)\n  #res <- optim(theta, varloglik, method=\"SANN\", control=list(fnscale=-1))\n  res <- optim(theta, varloglik, method=\"L-BFGS-B\", control=list(fnscale=-1), nstart=nstart)\n  \n  # store results in vectors\n  est.pars[ss,] <- res$par\n  loglik[ss] <- res$value\n}\n\nmse1<- mean((est.pars[,1] - (-1) )^2)\nmse2<- mean((est.pars[,2] - (5) )^2)\nmse1\nmse2\n\nbias1 <- mean(est.pars[,1]) - (-1)\nbias2 <- mean(est.pars[,2]) - 5\nbias1\nbias2\n\nbias1pct <- (mean(est.pars[,1]) - (-1))/(-1)\nbias2pct <- (mean(est.pars[,2]) - 5)/5\nbias1pct\nbias2pct\n\ncbind(est.pars[1:5,], loglik[1:5])\ncbind(est.pars, loglik)\n\nloglik[1:5,]\n",
    "created" : 1438625374001.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2714512121",
    "id" : "841FB128",
    "lastKnownWriteTime" : 1438631597,
    "path" : "C:/Users/amele1/Dropbox/peersmoking/programs/mc_bias_oldFeb2014.R",
    "project_path" : null,
    "properties" : {
        "notebook_format" : "pdf_document"
    },
    "source_on_save" : false,
    "type" : "r_source"
}